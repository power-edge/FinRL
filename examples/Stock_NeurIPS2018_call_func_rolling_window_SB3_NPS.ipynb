{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfv52r2G33jY"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/1-Introduction/Stock_NeurIPS2018_call_func_rolling_window_SB3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXaoZs2lh1hi"
   },
   "source": [
    "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading\n",
    "\n",
    "* **Pytorch Version** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGunVt8oLCVS"
   },
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sApkDlD9LIZv"
   },
   "source": [
    "<a id='0'></a>\n",
    "Task Discription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjLD2TZSLKZ-"
   },
   "source": [
    "We train a DRL agent for stock trading. This task is modeled as a Markov Decision Process (MDP), and the objective function is maximizing (expected) cumulative return.\n",
    "\n",
    "We specify the state-action-reward as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes many features and learns by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy\n",
    "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 consituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period.\n",
    "\n",
    "\n",
    "The data for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffsre789LY08"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Part 1. Install Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy5_PTmOh1hj"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 1.1. Install packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mPT0ipYE28wL",
    "outputId": "1cb18b7e-62aa-47de-b1e7-c37711d49555"
   },
   "outputs": [],
   "source": [
    "# ## install required packages\n",
    "# !pip install swig\n",
    "# !pip install wrds\n",
    "# !pip install pyportfolioopt\n",
    "## install finrl library\n",
    "# !pip install -q condacolab\n",
    "# import condacolab\n",
    "# condacolab.install()\n",
    "# !apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
    "# !pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGv01K8Sh1hn"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 1.2. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path = [os.path.dirname(p) if p.endswith(\"/FinRL/examples\") else p for p in sys.path]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lPqeTTwoh1hn",
    "outputId": "7918ded5-5571-4aa0-c335-e5ff1ba5a94e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/exchange_calendars/exchange_calendar.py:2347: FutureWarning: 'T' is deprecated and will be removed in a future version. Please use 'min' instead of 'T'.\n",
      "  align: pd.Timedelta | str = pd.Timedelta(1, \"T\"),\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/pyfolio/pos.py:26: UserWarning: Module \"zipline.assets\" not found; mutltipliers will not be applied to position notionals.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from finrl import config\n",
    "from finrl import config_tickers\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import DATA_SAVE_DIR\n",
    "from finrl.config import INDICATORS\n",
    "from finrl.config import RESULTS_DIR\n",
    "from finrl.config import TENSORBOARD_LOG_DIR\n",
    "from finrl.config import TEST_END_DATE\n",
    "from finrl.config import TEST_START_DATE\n",
    "from finrl.config import TRAINED_MODEL_DIR\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.meta.data_processor import DataProcessor\n",
    "from finrl.meta.data_processors.func import calc_train_trade_data\n",
    "from finrl.meta.data_processors.func import calc_train_trade_starts_ends_if_rolling\n",
    "from finrl.meta.data_processors.func import date2str\n",
    "from finrl.meta.data_processors.func import str2date\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.meta.preprocessor.preprocessors import data_split\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.plot import backtest_plot\n",
    "from finrl.plot import backtest_stats\n",
    "from finrl.plot import get_baseline\n",
    "from finrl.plot import get_daily_return\n",
    "from finrl.plot import plot_return\n",
    "from finrl.applications.stock_trading.stock_trading_rolling_window import stock_trading_rolling_window\n",
    "\n",
    "# sys.path.append(\"../FinRL\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2owTj985RW4"
   },
   "source": [
    "<a id='1.4'></a>\n",
    "# 2 Set parameters and run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finrl.config_tickers import DOW_30_TICKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RtUc_ofKmpdy",
    "outputId": "203fec48-d3fa-48fe-ec40-eda9a2799c48"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (101891, 8)\n",
      "Successfully added technical indicators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (3481, 8)\n",
      "Successfully added vix\n",
      "Successfully added turbulence index\n",
      "Stock Dimension: 29, State Space: 291\n",
      "num_subsets_if_rolling:  4\n",
      "train_starts:  ['2009-01-02', '2009-02-04', '2009-03-09', '2009-04-08']\n",
      "train_ends__:  ['2022-07-01', '2022-08-03', '2022-09-02', '2022-10-05']\n",
      "trade_starts:  ['2022-07-01', '2022-08-03', '2022-09-02', '2022-10-05']\n",
      "trade_ends__:  ['2022-08-03', '2022-09-02', '2022-10-05', '2022-10-28']\n",
      "i:  0\n",
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 112         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | -0.0212     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 10.3        |\n",
      "|    reward             | -0.13958913 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 0.416       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 109        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.486     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 32.4       |\n",
      "|    reward             | -0.9428561 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 1.89       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 106      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -160     |\n",
      "|    reward             | 4.570707 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 17.4     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -34.9      |\n",
      "|    reward             | -0.9078571 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.9        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 603        |\n",
      "|    reward             | -10.376651 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 323        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 188        |\n",
      "|    reward             | -36.240067 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 46.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -0.00889   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 107        |\n",
      "|    reward             | -0.5720585 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 9.43       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 84.4       |\n",
      "|    reward             | 0.34901845 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 6.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 261        |\n",
      "|    reward             | -2.5752969 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 51.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -99.4      |\n",
      "|    reward             | 0.95864624 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 12.7       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 104         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 52          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -92.4       |\n",
      "|    reward             | -0.28956547 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 5.68        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 105       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -181      |\n",
      "|    reward             | -1.577223 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 96.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 61         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 838        |\n",
      "|    reward             | -15.693515 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 444        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 104         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 66          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | 0.0196      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -37.5       |\n",
      "|    reward             | -0.17471461 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 2.8         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 104         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 71          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 128         |\n",
      "|    reward             | 0.060172513 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 20.5        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 104      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 76       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -40.9    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 82.8     |\n",
      "|    reward             | 4.187484 |\n",
      "|    std                | 0.993    |\n",
      "|    value_loss         | 12.2     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 313       |\n",
      "|    reward             | 0.7065084 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 90.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 30.9      |\n",
      "|    reward             | 0.6536253 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 4.98      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 104      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 90       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41      |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -246     |\n",
      "|    reward             | 5.652023 |\n",
      "|    std                | 0.995    |\n",
      "|    value_loss         | 41.2     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 95         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -322       |\n",
      "|    reward             | -21.224264 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 95.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 103       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 101       |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41       |\n",
      "|    explained_variance | 0.06      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 22.9      |\n",
      "|    reward             | 0.1638903 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 0.605     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 104         |\n",
      "|    iterations         | 2200        |\n",
      "|    time_elapsed       | 105         |\n",
      "|    total_timesteps    | 11000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2199        |\n",
      "|    policy_loss        | 214         |\n",
      "|    reward             | -0.82529294 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 29.7        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 110       |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0.00121   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -90.2     |\n",
      "|    reward             | 2.6277418 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 7.42      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 114       |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 140       |\n",
      "|    reward             | 2.9937925 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 21.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 105       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 118       |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | -10.5     |\n",
      "|    reward             | 3.4960737 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 29.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 105        |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 123        |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | -1.14e+03  |\n",
      "|    reward             | -24.577744 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 726        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 105      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 128      |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | -297     |\n",
      "|    reward             | 17.0267  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 293      |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 105         |\n",
      "|    iterations         | 2800        |\n",
      "|    time_elapsed       | 132         |\n",
      "|    total_timesteps    | 14000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2799        |\n",
      "|    policy_loss        | -138        |\n",
      "|    reward             | -0.77277225 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 18.8        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 105       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 137       |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | 130       |\n",
      "|    reward             | 0.2335051 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 15.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 143       |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | -457      |\n",
      "|    reward             | 2.3714335 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 133       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 104        |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 147        |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | -209       |\n",
      "|    reward             | 0.45095158 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 29.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 152       |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 214       |\n",
      "|    reward             | 3.0806599 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 36.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 104       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 157       |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | 246       |\n",
      "|    reward             | 5.1667676 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 64.2      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 103          |\n",
      "|    iterations         | 3400         |\n",
      "|    time_elapsed       | 163          |\n",
      "|    total_timesteps    | 17000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.3        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3399         |\n",
      "|    policy_loss        | -17          |\n",
      "|    reward             | -0.027677894 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 0.287        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 103         |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 169         |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | -1.19       |\n",
      "|    reward             | -0.07840123 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.659       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 102       |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 175       |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -65.8     |\n",
      "|    reward             | 0.8034206 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.57      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 102        |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 180        |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | -104       |\n",
      "|    reward             | -3.2285042 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 9.51       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 101      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 186      |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | -44.1    |\n",
      "|    reward             | 4.472754 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.53     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 102       |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 191       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 9.68e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | -1.35e+03 |\n",
      "|    reward             | 6.4434376 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.1e+03   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 101        |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 196        |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | -102       |\n",
      "|    reward             | -2.9005105 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 17.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 101       |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 202       |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -0.00612  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | -122      |\n",
      "|    reward             | 0.2925883 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 9.07      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 101      |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 207      |\n",
      "|    total_timesteps    | 21000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | -0.00105 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | -13.1    |\n",
      "|    reward             | 1.193953 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 2.63     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 101       |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 212       |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | -166      |\n",
      "|    reward             | 3.5501468 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 31.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 100        |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 217        |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | -92.8      |\n",
      "|    reward             | 0.28865796 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 8.94       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 100      |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 223      |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0.0455   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 170      |\n",
      "|    reward             | 3.809013 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 17.8     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 100       |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 229       |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | 146       |\n",
      "|    reward             | 3.477111  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 17.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 100       |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 234       |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | 167       |\n",
      "|    reward             | 6.4222565 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 52.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 100         |\n",
      "|    iterations         | 4800        |\n",
      "|    time_elapsed       | 239         |\n",
      "|    total_timesteps    | 24000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4799        |\n",
      "|    policy_loss        | 118         |\n",
      "|    reward             | -0.03794828 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 12.9        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 99        |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 245       |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -23.5     |\n",
      "|    reward             | 1.2999549 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 16.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 99         |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 250        |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | -67        |\n",
      "|    reward             | -3.0206902 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.93       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 99        |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 256       |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | 313       |\n",
      "|    reward             | 3.6755354 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 75.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 99         |\n",
      "|    iterations         | 5200       |\n",
      "|    time_elapsed       | 261        |\n",
      "|    total_timesteps    | 26000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5199       |\n",
      "|    policy_loss        | 272        |\n",
      "|    reward             | -0.3529694 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 48         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 99        |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 267       |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | -165      |\n",
      "|    reward             | 4.6813426 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 35.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 99       |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 272      |\n",
      "|    total_timesteps    | 27000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | 4.25     |\n",
      "|    reward             | 3.261797 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 5.67     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 98        |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 277       |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | 28.7      |\n",
      "|    reward             | 1.1952424 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.876     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 98        |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 283       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | 22.6      |\n",
      "|    reward             | 1.8348925 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.677     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 98          |\n",
      "|    iterations         | 5700        |\n",
      "|    time_elapsed       | 288         |\n",
      "|    total_timesteps    | 28500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5699        |\n",
      "|    policy_loss        | 291         |\n",
      "|    reward             | 0.025896277 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 50.1        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 98          |\n",
      "|    iterations         | 5800        |\n",
      "|    time_elapsed       | 293         |\n",
      "|    total_timesteps    | 29000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0.000507    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5799        |\n",
      "|    policy_loss        | -77.7       |\n",
      "|    reward             | -0.54821527 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 4.86        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 98          |\n",
      "|    iterations         | 5900        |\n",
      "|    time_elapsed       | 299         |\n",
      "|    total_timesteps    | 29500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5899        |\n",
      "|    policy_loss        | -411        |\n",
      "|    reward             | 0.096185714 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 112         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 98        |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 304       |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | -2.63e+03 |\n",
      "|    reward             | 71.955124 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 6.59e+03  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 98       |\n",
      "|    iterations         | 6100     |\n",
      "|    time_elapsed       | 309      |\n",
      "|    total_timesteps    | 30500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | -411     |\n",
      "|    reward             | 8.628677 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 534      |\n",
      "------------------------------------\n",
      "day: 3396, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9729280.08\n",
      "total_reward: 8729280.08\n",
      "total_cost: 25619.49\n",
      "total_trades: 56639\n",
      "Sharpe: 0.956\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 98          |\n",
      "|    iterations         | 6200        |\n",
      "|    time_elapsed       | 314         |\n",
      "|    total_timesteps    | 31000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | -0.000517   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6199        |\n",
      "|    policy_loss        | 85          |\n",
      "|    reward             | -0.40289187 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 11.2        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 98       |\n",
      "|    iterations         | 6300     |\n",
      "|    time_elapsed       | 319      |\n",
      "|    total_timesteps    | 31500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | -56      |\n",
      "|    reward             | 4.176911 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 2.92     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 98         |\n",
      "|    iterations         | 6400       |\n",
      "|    time_elapsed       | 324        |\n",
      "|    total_timesteps    | 32000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6399       |\n",
      "|    policy_loss        | 63.8       |\n",
      "|    reward             | -0.2829822 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 7.87       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 98        |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 329       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | -119      |\n",
      "|    reward             | 1.7631344 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 13.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 98        |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 335       |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | 105       |\n",
      "|    reward             | 2.9837341 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 10.5      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 98           |\n",
      "|    iterations         | 6700         |\n",
      "|    time_elapsed       | 340          |\n",
      "|    total_timesteps    | 33500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.6        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6699         |\n",
      "|    policy_loss        | -367         |\n",
      "|    reward             | 0.0039635133 |\n",
      "|    std                | 1.02         |\n",
      "|    value_loss         | 179          |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 98         |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 346        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 87.8       |\n",
      "|    reward             | -3.5046165 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 6.74       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 98         |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 351        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | 61.5       |\n",
      "|    reward             | -1.1070362 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.32       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 98         |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 356        |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | -5.23      |\n",
      "|    reward             | 0.23230296 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.14       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 362       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 92.7      |\n",
      "|    reward             | 3.4150748 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 9.33      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 368        |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | -87.6      |\n",
      "|    reward             | -2.1253817 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.92       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 97       |\n",
      "|    iterations         | 7300     |\n",
      "|    time_elapsed       | 372      |\n",
      "|    total_timesteps    | 36500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7299     |\n",
      "|    policy_loss        | 115      |\n",
      "|    reward             | 2.797872 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 14.1     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 7400       |\n",
      "|    time_elapsed       | 377        |\n",
      "|    total_timesteps    | 37000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0.000561   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7399       |\n",
      "|    policy_loss        | 247        |\n",
      "|    reward             | -4.8869557 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 49.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 98        |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 382       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0.123     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | -133      |\n",
      "|    reward             | 4.2959857 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 13.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 98         |\n",
      "|    iterations         | 7600       |\n",
      "|    time_elapsed       | 386        |\n",
      "|    total_timesteps    | 38000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7599       |\n",
      "|    policy_loss        | 126        |\n",
      "|    reward             | -1.2232757 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 10.8       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 98       |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 391      |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | 186      |\n",
      "|    reward             | 3.606281 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 23.1     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 98        |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 397       |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7799      |\n",
      "|    policy_loss        | 107       |\n",
      "|    reward             | 1.2702245 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 22.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 98         |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 402        |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | -113       |\n",
      "|    reward             | 0.36042202 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8.01       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 98        |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 407       |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | 156       |\n",
      "|    reward             | 6.5918713 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 34.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 98         |\n",
      "|    iterations         | 8100       |\n",
      "|    time_elapsed       | 413        |\n",
      "|    total_timesteps    | 40500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -0.157     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8099       |\n",
      "|    policy_loss        | 35.7       |\n",
      "|    reward             | -5.6606073 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 25.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 98         |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 418        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0.000175   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | 3.59       |\n",
      "|    reward             | 0.90879875 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 98         |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 423        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -0.00261   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | -193       |\n",
      "|    reward             | -0.7457849 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 33         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 98         |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 428        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | 98.3       |\n",
      "|    reward             | 0.08123343 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 6.48       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 98         |\n",
      "|    iterations         | 8500       |\n",
      "|    time_elapsed       | 432        |\n",
      "|    total_timesteps    | 42500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8499       |\n",
      "|    policy_loss        | -5.29      |\n",
      "|    reward             | -1.3616253 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 18.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 98         |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 437        |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | -189       |\n",
      "|    reward             | -1.9754716 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 21.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 98        |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 443       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | 353       |\n",
      "|    reward             | -5.338242 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 79.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 98        |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 448       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | -0.00185  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | 521       |\n",
      "|    reward             | -5.718752 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 161       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 454       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 1.85e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | -15.7     |\n",
      "|    reward             | 5.8566365 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4.86      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 459        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0.00763    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | -55.9      |\n",
      "|    reward             | 0.33314496 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 9100       |\n",
      "|    time_elapsed       | 465        |\n",
      "|    total_timesteps    | 45500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9099       |\n",
      "|    policy_loss        | 13         |\n",
      "|    reward             | 0.44753134 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.485      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 97          |\n",
      "|    iterations         | 9200        |\n",
      "|    time_elapsed       | 470         |\n",
      "|    total_timesteps    | 46000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9199        |\n",
      "|    policy_loss        | 157         |\n",
      "|    reward             | -0.10188296 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 15.3        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 9300       |\n",
      "|    time_elapsed       | 475        |\n",
      "|    total_timesteps    | 46500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9299       |\n",
      "|    policy_loss        | 116        |\n",
      "|    reward             | -1.1688141 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 21.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 480       |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | 926       |\n",
      "|    reward             | -4.092865 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 688       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 97       |\n",
      "|    iterations         | 9500     |\n",
      "|    time_elapsed       | 485      |\n",
      "|    total_timesteps    | 47500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | -107     |\n",
      "|    reward             | 1.717489 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 61.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 97       |\n",
      "|    iterations         | 9600     |\n",
      "|    time_elapsed       | 490      |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 0.0061   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | 3.32     |\n",
      "|    reward             | 3.031297 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.538    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 495       |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | -31.9     |\n",
      "|    reward             | 1.629114  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.912     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 97        |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 500       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 110       |\n",
      "|    reward             | -5.234526 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 8.68      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 506        |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0.0305     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | -243       |\n",
      "|    reward             | 0.30411556 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 42.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 97         |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 512        |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 14.9       |\n",
      "|    reward             | -0.6443792 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.74       |\n",
      "--------------------------------------\n",
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 45         |\n",
      "|    time_elapsed    | 299        |\n",
      "|    total_timesteps | 13588      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 78.6       |\n",
      "|    critic_loss     | 212        |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 10191      |\n",
      "|    reward          | -4.7601366 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 8          |\n",
      "|    fps             | 41         |\n",
      "|    time_elapsed    | 650        |\n",
      "|    total_timesteps | 27176      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 28.8       |\n",
      "|    critic_loss     | 8.02       |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 23779      |\n",
      "|    reward          | -4.7601366 |\n",
      "-----------------------------------\n",
      "day: 3396, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6030115.26\n",
      "total_reward: 5030115.26\n",
      "total_cost: 999.00\n",
      "total_trades: 61077\n",
      "Sharpe: 0.812\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 12         |\n",
      "|    fps             | 41         |\n",
      "|    time_elapsed    | 976        |\n",
      "|    total_timesteps | 40764      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 7.59       |\n",
      "|    critic_loss     | 4.73       |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 37367      |\n",
      "|    reward          | -4.7601366 |\n",
      "-----------------------------------\n",
      "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 0.0001, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 102        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 19         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.17343867 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 42           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0112612955 |\n",
      "|    clip_fraction        | 0.134        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.2        |\n",
      "|    explained_variance   | 0.00311      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 7.71         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.02        |\n",
      "|    reward               | -0.046515137 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 13.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012530003 |\n",
      "|    clip_fraction        | 0.0895      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00103    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 64.4        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    reward               | -4.5328546  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 125         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010822201 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.1       |\n",
      "|    explained_variance   | -0.00736    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 18.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    reward               | -0.98009515 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 41.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011951691 |\n",
      "|    clip_fraction        | 0.0939      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.1       |\n",
      "|    explained_variance   | 0.00241     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 74.8        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    reward               | -0.33283207 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 118         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01037426  |\n",
      "|    clip_fraction        | 0.0693      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.1       |\n",
      "|    explained_variance   | 0.00177     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 154         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    reward               | -0.14440228 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 272         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012628544 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.1       |\n",
      "|    explained_variance   | -0.00886    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 13.9        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    reward               | 3.9190924   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 30.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017854206 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00464    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 128         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    reward               | 3.0119522   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 179         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013785381 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.00591     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 18          |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    reward               | -0.94726753 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 38.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 198         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013234386 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.0142      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 49.2        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0194     |\n",
      "|    reward               | 1.7428368   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 82.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 103          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 217          |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.012176381  |\n",
      "|    clip_fraction        | 0.108        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.2        |\n",
      "|    explained_variance   | -0.00555     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 92.7         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.0147      |\n",
      "|    reward               | -0.007549846 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 132          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 236         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017251605 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.0192      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    reward               | -0.2719362  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 23          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 255         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012174053 |\n",
      "|    clip_fraction        | 0.0903      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.000373   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 154         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    reward               | 5.8833337   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 274         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 275         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015000214 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00078    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 49.2        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    reward               | 1.7052525   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "day: 3396, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5375263.22\n",
      "total_reward: 4375263.22\n",
      "total_cost: 433353.02\n",
      "total_trades: 92787\n",
      "Sharpe: 0.823\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010829195 |\n",
      "|    clip_fraction        | 0.0845      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.017       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 29          |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    reward               | 0.15332948  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 124         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 315         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012684219 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.0186      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 51.2        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    reward               | -0.23950538 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 338         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01730892  |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.0189      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 11.4        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0182     |\n",
      "|    reward               | -0.18972686 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 26.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 361         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012075163 |\n",
      "|    clip_fraction        | 0.0872      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.0106      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 97          |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    reward               | 0.4587822   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 221         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 380         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009687699 |\n",
      "|    clip_fraction        | 0.0736      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0149     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 35.7        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0105     |\n",
      "|    reward               | 0.310881    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 401         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015348718 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0216     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 40.5        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    reward               | -1.0759853  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 113         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 422         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013462104 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0108      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 55.8        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    reward               | 6.0933056   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 140         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 443         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016791847 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.00395    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 10.2        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    reward               | -1.6831037  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 22.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 464         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017295372 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.00851    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 76.6        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    reward               | -0.79713184 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 208         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 482         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013910914 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00118     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 28.8        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    reward               | -2.398945   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 68.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 503         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018150445 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0149      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 19.4        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    reward               | 0.54644114  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 51.1        |\n",
      "-----------------------------------------\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.00015, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to results/sac\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 39        |\n",
      "|    time_elapsed    | 347       |\n",
      "|    total_timesteps | 13588     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 228       |\n",
      "|    critic_loss     | 114       |\n",
      "|    ent_coef        | 0.0536    |\n",
      "|    ent_coef_loss   | -139      |\n",
      "|    learning_rate   | 0.00015   |\n",
      "|    n_updates       | 13487     |\n",
      "|    reward          | -4.719631 |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 8          |\n",
      "|    fps             | 37         |\n",
      "|    time_elapsed    | 721        |\n",
      "|    total_timesteps | 27176      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 93.1       |\n",
      "|    critic_loss     | 528        |\n",
      "|    ent_coef        | 0.00738    |\n",
      "|    ent_coef_loss   | -157       |\n",
      "|    learning_rate   | 0.00015    |\n",
      "|    n_updates       | 27075      |\n",
      "|    reward          | -10.452657 |\n",
      "-----------------------------------\n",
      "day: 3396, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11695219.09\n",
      "total_reward: 10695219.09\n",
      "total_cost: 7545.26\n",
      "total_trades: 70705\n",
      "Sharpe: 1.005\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 36        |\n",
      "|    time_elapsed    | 1103      |\n",
      "|    total_timesteps | 40764     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 41.9      |\n",
      "|    critic_loss     | 44.9      |\n",
      "|    ent_coef        | 0.00223   |\n",
      "|    ent_coef_loss   | -1.14     |\n",
      "|    learning_rate   | 0.00015   |\n",
      "|    n_updates       | 40663     |\n",
      "|    reward          | -8.535228 |\n",
      "----------------------------------\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0008}\n",
      "Using cpu device\n",
      "Logging to results/td3\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 44        |\n",
      "|    time_elapsed    | 308       |\n",
      "|    total_timesteps | 13588     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 161       |\n",
      "|    critic_loss     | 1.49e+04  |\n",
      "|    learning_rate   | 0.0008    |\n",
      "|    n_updates       | 10191     |\n",
      "|    reward          | -5.987072 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 39        |\n",
      "|    time_elapsed    | 696       |\n",
      "|    total_timesteps | 27176     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 70.7      |\n",
      "|    critic_loss     | 962       |\n",
      "|    learning_rate   | 0.0008    |\n",
      "|    n_updates       | 23779     |\n",
      "|    reward          | -5.987072 |\n",
      "----------------------------------\n",
      "day: 3396, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7273909.02\n",
      "total_reward: 6273909.02\n",
      "total_cost: 999.00\n",
      "total_trades: 64499\n",
      "Sharpe: 0.840\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 38        |\n",
      "|    time_elapsed    | 1064      |\n",
      "|    total_timesteps | 40764     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 54.3      |\n",
      "|    critic_loss     | 144       |\n",
      "|    learning_rate   | 0.0008    |\n",
      "|    n_updates       | 37367     |\n",
      "|    reward          | -5.987072 |\n",
      "----------------------------------\n",
      "hit end!\n",
      "hit end!\n",
      "hit end!\n",
      "hit end!\n",
      "hit end!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (22, 8)\n",
      "i:  1\n",
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 63         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0527     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -76.8      |\n",
      "|    reward             | -1.0965456 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.38       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -0.275    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 27.7      |\n",
      "|    reward             | 2.0942023 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.604     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -16.2     |\n",
      "|    reward             | 1.1556073 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 8.64      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 64         |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0.0605     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 176        |\n",
      "|    reward             | 0.65628016 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 25.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 66        |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0.0396    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -368      |\n",
      "|    reward             | 5.2127266 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 97.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -5.3      |\n",
      "|    reward             | 5.0732203 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 12.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 67        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -0.276    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 29.2      |\n",
      "|    reward             | 0.5227703 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.89      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 59         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 196        |\n",
      "|    reward             | -1.4289927 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 26.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 67         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 66         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 175        |\n",
      "|    reward             | -1.2073653 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 25.1       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 68       |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 72       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 372      |\n",
      "|    reward             | -3.68106 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 87.1     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 68        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 213       |\n",
      "|    reward             | 5.183153  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 39.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 69        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -0.00468  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -571      |\n",
      "|    reward             | 12.316115 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 374       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 69         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 92         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -556       |\n",
      "|    reward             | -2.1083696 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 191        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 70        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 99        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -9.54e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -124      |\n",
      "|    reward             | 0.55155   |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 16.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 70         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 105        |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 91.6       |\n",
      "|    reward             | 0.17371449 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.83       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 112        |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 41         |\n",
      "|    reward             | -6.0573874 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.53       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 119        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | -0.0287    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -590       |\n",
      "|    reward             | 0.68784004 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 301        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 71        |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 125       |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 0.889     |\n",
      "|    reward             | 15.730074 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.73      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 132        |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -446       |\n",
      "|    reward             | -10.884906 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 160        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 71         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 139        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 742        |\n",
      "|    reward             | -3.4815366 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 358        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 145       |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 152       |\n",
      "|    reward             | 2.5064495 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 18.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 72        |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 151       |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | 73.6      |\n",
      "|    reward             | 1.9712079 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.35      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 158        |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | -126       |\n",
      "|    reward             | -3.6891627 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 10.9       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 163      |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 261      |\n",
      "|    reward             | 1.740278 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 40.3     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 73       |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 169      |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | -40.5    |\n",
      "|    reward             | 9.247895 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 8.67     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 175        |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | 196        |\n",
      "|    reward             | -1.0859381 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 269        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 180        |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | 540        |\n",
      "|    reward             | -3.3023732 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 197        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 74         |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 187        |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | 30.2       |\n",
      "|    reward             | -2.5096033 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.8        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 193       |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | 17.3      |\n",
      "|    reward             | 0.7298215 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.585     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 74       |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 200      |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 6.17     |\n",
      "|    reward             | 1.8259   |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 10.4     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 206       |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | 96.4      |\n",
      "|    reward             | 0.6356246 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 8         |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 75       |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 212      |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | -0.0452  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -576     |\n",
      "|    reward             | 8.987734 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 189      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 75        |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 217       |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | 163       |\n",
      "|    reward             | 5.5470304 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 37.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 3400       |\n",
      "|    time_elapsed       | 223        |\n",
      "|    total_timesteps    | 17000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3399       |\n",
      "|    policy_loss        | 34.7       |\n",
      "|    reward             | 0.18180986 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.01       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 229        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 12         |\n",
      "|    reward             | -0.5433818 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.342      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 76       |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 235      |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | -12.4    |\n",
      "|    reward             | 1.047733 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.255    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 240        |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -0.00174   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | 148        |\n",
      "|    reward             | -1.6908395 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 16.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 246       |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | -0.243    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 43.5      |\n",
      "|    reward             | 2.8155036 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.4       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 252        |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | 453        |\n",
      "|    reward             | -0.5614919 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 112        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 259        |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -0.000327  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | 326        |\n",
      "|    reward             | 0.37516388 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 67.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 265       |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0.0301    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | -108      |\n",
      "|    reward             | 1.2266632 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 11.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 76          |\n",
      "|    iterations         | 4200        |\n",
      "|    time_elapsed       | 273         |\n",
      "|    total_timesteps    | 21000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4199        |\n",
      "|    policy_loss        | 24.2        |\n",
      "|    reward             | -0.38331947 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.535       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 76         |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 279        |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | 194        |\n",
      "|    reward             | -2.4510586 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 25         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 76        |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 286       |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | -306      |\n",
      "|    reward             | 1.1109331 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 96.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 291       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 119       |\n",
      "|    reward             | 1.9972166 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 8.45      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 77       |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 297      |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | -51.7    |\n",
      "|    reward             | 8.620819 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 9.86     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 302        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | 923        |\n",
      "|    reward             | -7.3927402 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 644        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 77       |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 308      |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 112      |\n",
      "|    reward             | 2.631262 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 9.71     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 4900       |\n",
      "|    time_elapsed       | 314        |\n",
      "|    total_timesteps    | 24500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4899       |\n",
      "|    policy_loss        | 209        |\n",
      "|    reward             | -2.2404857 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 27.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 320       |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0.00562   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | 37.7      |\n",
      "|    reward             | 0.9680422 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.64      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 326        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 251        |\n",
      "|    reward             | 0.94811416 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 47.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 333       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | 196       |\n",
      "|    reward             | 6.9018664 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 31.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 339       |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | 298       |\n",
      "|    reward             | 11.914826 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 194       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 344        |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | 2.44e+03   |\n",
      "|    reward             | -13.945244 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.26e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 350        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -0.0552    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | -68.2      |\n",
      "|    reward             | -3.0389862 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.6        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 78       |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 357      |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | 2.83     |\n",
      "|    reward             | 1.567999 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 2.03     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 362       |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | 36.4      |\n",
      "|    reward             | 1.0116543 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 16.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 5800       |\n",
      "|    time_elapsed       | 368        |\n",
      "|    total_timesteps    | 29000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5799       |\n",
      "|    policy_loss        | 292        |\n",
      "|    reward             | -1.1498884 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 58.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 374       |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -451      |\n",
      "|    reward             | 1.8411303 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 132       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 380       |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | 377       |\n",
      "|    reward             | 2.5411417 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 85.3      |\n",
      "-------------------------------------\n",
      "day: 3374, episode: 10\n",
      "begin_total_asset: 1065270.86\n",
      "end_total_asset: 9745137.29\n",
      "total_reward: 8679866.43\n",
      "total_cost: 98314.57\n",
      "total_trades: 63804\n",
      "Sharpe: 1.005\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 78          |\n",
      "|    iterations         | 6100        |\n",
      "|    time_elapsed       | 386         |\n",
      "|    total_timesteps    | 30500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | -0.00131    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6099        |\n",
      "|    policy_loss        | 44          |\n",
      "|    reward             | -0.13817099 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 4.26        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 79          |\n",
      "|    iterations         | 6200        |\n",
      "|    time_elapsed       | 391         |\n",
      "|    total_timesteps    | 31000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6199        |\n",
      "|    policy_loss        | 124         |\n",
      "|    reward             | -0.68858236 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 9.24        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 79        |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 397       |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0.284     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 25.4      |\n",
      "|    reward             | 1.0255948 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.477     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 6400       |\n",
      "|    time_elapsed       | 405        |\n",
      "|    total_timesteps    | 32000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6399       |\n",
      "|    policy_loss        | 248        |\n",
      "|    reward             | -3.1899688 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 46.8       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 78       |\n",
      "|    iterations         | 6500     |\n",
      "|    time_elapsed       | 412      |\n",
      "|    total_timesteps    | 32500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | 42.2     |\n",
      "|    reward             | 4.023519 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 3.65     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 418       |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | 489       |\n",
      "|    reward             | 3.4709833 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 151       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 425       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | 620       |\n",
      "|    reward             | 2.0492032 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 228       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 431       |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -185      |\n",
      "|    reward             | 1.4511054 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 32.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 437        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | 42.1       |\n",
      "|    reward             | -1.0403068 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.62       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 444       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | -0.000401 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | 104       |\n",
      "|    reward             | -1.201881 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 13.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 78          |\n",
      "|    iterations         | 7100        |\n",
      "|    time_elapsed       | 450         |\n",
      "|    total_timesteps    | 35500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | -0.00324    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7099        |\n",
      "|    policy_loss        | -278        |\n",
      "|    reward             | -0.82614607 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 127         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 457       |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | 416       |\n",
      "|    reward             | 5.4825764 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 143       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 464       |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | 291       |\n",
      "|    reward             | 10.421724 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 83.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 78       |\n",
      "|    iterations         | 7400     |\n",
      "|    time_elapsed       | 470      |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7399     |\n",
      "|    policy_loss        | 1.66e+03 |\n",
      "|    reward             | -7.71128 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 2.04e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 476       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | 138       |\n",
      "|    reward             | 3.3002546 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 13.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 7600       |\n",
      "|    time_elapsed       | 482        |\n",
      "|    total_timesteps    | 38000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7599       |\n",
      "|    policy_loss        | 136        |\n",
      "|    reward             | -4.0815797 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 12.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 488       |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | 5.87      |\n",
      "|    reward             | 2.0746865 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.37      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 495       |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | -0.0533   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7799      |\n",
      "|    policy_loss        | 116       |\n",
      "|    reward             | 1.4306306 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 10.3      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 78       |\n",
      "|    iterations         | 7900     |\n",
      "|    time_elapsed       | 502      |\n",
      "|    total_timesteps    | 39500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | -0.0178  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | 114      |\n",
      "|    reward             | 6.890039 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 18.7     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 509       |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | 438       |\n",
      "|    reward             | 21.555187 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 273       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 515       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | 3.33e+03  |\n",
      "|    reward             | -25.69684 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.99e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 522        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | -49.7      |\n",
      "|    reward             | -3.0340161 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.36       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 529       |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | 44.8      |\n",
      "|    reward             | 0.8192512 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.8       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 536        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | 127        |\n",
      "|    reward             | 0.36511976 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 34.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 542       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | 329       |\n",
      "|    reward             | 0.7734145 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 66        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 549       |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | -1.25e+03 |\n",
      "|    reward             | 1.3205905 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 987       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 555       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | 2.79e+03  |\n",
      "|    reward             | 15.1368   |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.03e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 562       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | -78.3     |\n",
      "|    reward             | 0.2910617 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.39      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 8900       |\n",
      "|    time_elapsed       | 568        |\n",
      "|    total_timesteps    | 44500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8899       |\n",
      "|    policy_loss        | 170        |\n",
      "|    reward             | -0.2892696 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 20.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 576        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | -134       |\n",
      "|    reward             | -0.0742398 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 13.5       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 78       |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 582      |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | 252      |\n",
      "|    reward             | -2.68159 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 57.9     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 77       |\n",
      "|    iterations         | 9200     |\n",
      "|    time_elapsed       | 589      |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | -144     |\n",
      "|    reward             | 3.981625 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 18.4     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 9300       |\n",
      "|    time_elapsed       | 596        |\n",
      "|    total_timesteps    | 46500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9299       |\n",
      "|    policy_loss        | 443        |\n",
      "|    reward             | -1.0705748 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 129        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 603       |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | 359       |\n",
      "|    reward             | 5.8150363 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 91.6      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 77       |\n",
      "|    iterations         | 9500     |\n",
      "|    time_elapsed       | 609      |\n",
      "|    total_timesteps    | 47500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | -288     |\n",
      "|    reward             | 2.439928 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 62.5     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 615        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | 82.8       |\n",
      "|    reward             | -0.9825288 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.56       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 77         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 621        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | -19.9      |\n",
      "|    reward             | -2.1765954 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.4        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 77        |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 628       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | -0.0629   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | -75.3     |\n",
      "|    reward             | 1.9356959 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 6.95      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 9900      |\n",
      "|    time_elapsed       | 633       |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | 41.5      |\n",
      "|    reward             | 1.8094997 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.69      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 78        |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 639       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | -19.8     |\n",
      "|    reward             | 1.4406337 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.36      |\n",
      "-------------------------------------\n",
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 37        |\n",
      "|    time_elapsed    | 360       |\n",
      "|    total_timesteps | 13500     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -34.2     |\n",
      "|    critic_loss     | 42.2      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 10125     |\n",
      "|    reward          | -3.886443 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 33        |\n",
      "|    time_elapsed    | 811       |\n",
      "|    total_timesteps | 27000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -20.2     |\n",
      "|    critic_loss     | 9.08      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 23625     |\n",
      "|    reward          | -3.886443 |\n",
      "----------------------------------\n",
      "day: 3374, episode: 10\n",
      "begin_total_asset: 1035607.70\n",
      "end_total_asset: 6776070.60\n",
      "total_reward: 5740462.90\n",
      "total_cost: 1034.57\n",
      "total_trades: 43862\n",
      "Sharpe: 0.882\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 31        |\n",
      "|    time_elapsed    | 1287      |\n",
      "|    total_timesteps | 40500     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -16.2     |\n",
      "|    critic_loss     | 5         |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 37125     |\n",
      "|    reward          | -3.886443 |\n",
      "----------------------------------\n",
      "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 0.0001, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 70          |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 29          |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.92267156 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010216765 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00946    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 6.04        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0212     |\n",
      "|    reward               | -1.4959456  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 14.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010086159 |\n",
      "|    clip_fraction        | 0.0925      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.000393   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 73.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    reward               | 5.021274    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 116         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 75          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011844812 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.00324     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 12.2        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    reward               | 0.57936573  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 24.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 138          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0140391085 |\n",
      "|    clip_fraction        | 0.126        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.2        |\n",
      "|    explained_variance   | -0.00323     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 19.1         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0164      |\n",
      "|    reward               | 0.3512653    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 61.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012276353 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.000239   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 41          |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    reward               | -0.14297232 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010978796 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0966     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 5.94        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    reward               | -1.9651715  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 20.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 75           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 218          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0094618015 |\n",
      "|    clip_fraction        | 0.0801       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.3        |\n",
      "|    explained_variance   | 0.0013       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 132          |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.011       |\n",
      "|    reward               | 1.4985706    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 192          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 241         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010457786 |\n",
      "|    clip_fraction        | 0.0806      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.00886    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 31.9        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00974    |\n",
      "|    reward               | 0.283191    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 81.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 76          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 266         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018252335 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.017      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 26.1        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    reward               | 1.1014158   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 72.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 290         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015990956 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00168     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 80.8        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    reward               | 2.1768463   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 138         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 77          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 316         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014164675 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0549     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 8.93        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0184     |\n",
      "|    reward               | -0.7266498  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 17.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 78          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 337         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015570427 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00655     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 45.9        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    reward               | -2.2002172  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 109         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 362         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01531346  |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.0245      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    reward               | 0.081977725 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 89.3        |\n",
      "-----------------------------------------\n",
      "day: 3374, episode: 10\n",
      "begin_total_asset: 1030174.53\n",
      "end_total_asset: 5108964.67\n",
      "total_reward: 4078790.15\n",
      "total_cost: 409243.12\n",
      "total_trades: 90700\n",
      "Sharpe: 0.803\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 386         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016305365 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.00314     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 15.6        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    reward               | 0.3001319   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 73.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 79          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 410         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017158624 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.0109      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 37.7        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    reward               | 1.0079628   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 80         |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 434        |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01586061 |\n",
      "|    clip_fraction        | 0.158      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.4      |\n",
      "|    explained_variance   | -0.0534    |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 8.78       |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.015     |\n",
      "|    reward               | -1.5894057 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 29.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 458         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015390649 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.00125     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 69.9        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0174     |\n",
      "|    reward               | -8.769024   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 150         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 483         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013092267 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.00726    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 43.5        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    reward               | 3.4919398   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 95          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 80          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 505         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015183963 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.00272    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 40.7        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    reward               | 2.4929485   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 52.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 81          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 527         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013076955 |\n",
      "|    clip_fraction        | 0.0991      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.00845    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 63.1        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    reward               | 0.765114    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 163         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 549         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013276296 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.0349     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 23.4        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    reward               | 0.9194717   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 41.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 570         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013034154 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | -0.0123     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 154         |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    reward               | -5.7496543  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 227         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 82          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 592         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016473068 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | -0.00487    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 40          |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    reward               | -0.7541023  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 99          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 613         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017274518 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0.00725     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0181     |\n",
      "|    reward               | -0.57936335 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 39.6        |\n",
      "-----------------------------------------\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.00015, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to results/sac\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 33        |\n",
      "|    time_elapsed    | 406       |\n",
      "|    total_timesteps | 13500     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 189       |\n",
      "|    critic_loss     | 6.65e+03  |\n",
      "|    ent_coef        | 0.0373    |\n",
      "|    ent_coef_loss   | -155      |\n",
      "|    learning_rate   | 0.00015   |\n",
      "|    n_updates       | 13399     |\n",
      "|    reward          | -6.141515 |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 8          |\n",
      "|    fps             | 32         |\n",
      "|    time_elapsed    | 829        |\n",
      "|    total_timesteps | 27000      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 65.5       |\n",
      "|    critic_loss     | 19.1       |\n",
      "|    ent_coef        | 0.00528    |\n",
      "|    ent_coef_loss   | -139       |\n",
      "|    learning_rate   | 0.00015    |\n",
      "|    n_updates       | 26899      |\n",
      "|    reward          | -5.6313553 |\n",
      "-----------------------------------\n",
      "day: 3374, episode: 10\n",
      "begin_total_asset: 1045904.28\n",
      "end_total_asset: 4813283.59\n",
      "total_reward: 3767379.31\n",
      "total_cost: 2341.56\n",
      "total_trades: 48578\n",
      "Sharpe: 0.620\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 33        |\n",
      "|    time_elapsed    | 1199      |\n",
      "|    total_timesteps | 40500     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 54.7      |\n",
      "|    critic_loss     | 34.8      |\n",
      "|    ent_coef        | 0.00277   |\n",
      "|    ent_coef_loss   | 13.6      |\n",
      "|    learning_rate   | 0.00015   |\n",
      "|    n_updates       | 40399     |\n",
      "|    reward          | -6.985569 |\n",
      "----------------------------------\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0008}\n",
      "Using cpu device\n",
      "Logging to results/td3\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 55         |\n",
      "|    time_elapsed    | 241        |\n",
      "|    total_timesteps | 13500      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 190        |\n",
      "|    critic_loss     | 5.34e+03   |\n",
      "|    learning_rate   | 0.0008     |\n",
      "|    n_updates       | 10125      |\n",
      "|    reward          | -7.5881934 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 8          |\n",
      "|    fps             | 51         |\n",
      "|    time_elapsed    | 522        |\n",
      "|    total_timesteps | 27000      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 77.6       |\n",
      "|    critic_loss     | 985        |\n",
      "|    learning_rate   | 0.0008     |\n",
      "|    n_updates       | 23625      |\n",
      "|    reward          | -7.5881934 |\n",
      "-----------------------------------\n",
      "day: 3374, episode: 10\n",
      "begin_total_asset: 1046748.65\n",
      "end_total_asset: 7603571.73\n",
      "total_reward: 6556823.08\n",
      "total_cost: 1429.08\n",
      "total_trades: 47302\n",
      "Sharpe: 0.911\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 12         |\n",
      "|    fps             | 49         |\n",
      "|    time_elapsed    | 813        |\n",
      "|    total_timesteps | 40500      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 54.2       |\n",
      "|    critic_loss     | 70.1       |\n",
      "|    learning_rate   | 0.0008     |\n",
      "|    n_updates       | 37125      |\n",
      "|    reward          | -7.5881934 |\n",
      "-----------------------------------\n",
      "hit end!\n",
      "hit end!\n",
      "hit end!\n",
      "hit end!\n",
      "hit end!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (22, 8)\n",
      "i:  2\n",
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 72         |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.211     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -155       |\n",
      "|    reward             | 0.54738414 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 17.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 74        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | -0.0618   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -35       |\n",
      "|    reward             | 3.5681946 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 1.25      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 78         |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.0364    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | 203        |\n",
      "|    reward             | -1.1559656 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 25.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 82        |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0.178     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 23.6      |\n",
      "|    reward             | 2.1390991 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 2.3       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 84       |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.1    |\n",
      "|    explained_variance | 0.00813  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 326      |\n",
      "|    reward             | 4.264452 |\n",
      "|    std                | 0.999    |\n",
      "|    value_loss         | 74.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 85       |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | -0.00748 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -639     |\n",
      "|    reward             | 1.487841 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 269      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 86       |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 40       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0.0554   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 13.2     |\n",
      "|    reward             | 1.643945 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 0.775    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 87        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | -0.0688   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -349      |\n",
      "|    reward             | 1.1444759 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 75.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 87         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 51         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -0.0463    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 81.4       |\n",
      "|    reward             | 0.86878604 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.4        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 88         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 56         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.098      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 115        |\n",
      "|    reward             | -2.5287232 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 8.06       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 88         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 62         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -0.00429   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -106       |\n",
      "|    reward             | 0.94445914 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 10.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 88        |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 397       |\n",
      "|    reward             | 3.4012196 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 108       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 73         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | -0.0171    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 382        |\n",
      "|    reward             | -0.9985231 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 105        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 78         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -104       |\n",
      "|    reward             | -2.8948298 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 8.04       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 83         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -9.16      |\n",
      "|    reward             | 0.06713656 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.3        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 89          |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 89          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.4       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 94.7        |\n",
      "|    reward             | -0.27250856 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 7.75        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 95         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0.00134    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 61.2       |\n",
      "|    reward             | -10.645115 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.1        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 88         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.0144     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 1.84       |\n",
      "|    reward             | 0.46119294 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 22.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 88        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 107       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 1.7e+03   |\n",
      "|    reward             | 25.251045 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.02e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 88        |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 850       |\n",
      "|    reward             | -8.265443 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 557       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 87         |\n",
      "|    iterations         | 2100       |\n",
      "|    time_elapsed       | 119        |\n",
      "|    total_timesteps    | 10500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.191      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2099       |\n",
      "|    policy_loss        | -28.4      |\n",
      "|    reward             | 0.14898475 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.963      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 87         |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 125        |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 242        |\n",
      "|    reward             | 0.88226235 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 39.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 87         |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 131        |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | -0.144     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | 180        |\n",
      "|    reward             | -1.1240717 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 31.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 87         |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 137        |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0.00615    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 223        |\n",
      "|    reward             | -2.5651917 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 52.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 87         |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 143        |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | 504        |\n",
      "|    reward             | -3.9539263 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 134        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 86        |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 149       |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | 411       |\n",
      "|    reward             | 6.2348266 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 174       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 87         |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 154        |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | -0.00937   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | -105       |\n",
      "|    reward             | 0.33409292 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 9.21       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 87        |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 159       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 28.1      |\n",
      "|    reward             | 1.4208876 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.73      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 88         |\n",
      "|    iterations         | 2900       |\n",
      "|    time_elapsed       | 164        |\n",
      "|    total_timesteps    | 14500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2899       |\n",
      "|    policy_loss        | 139        |\n",
      "|    reward             | 0.29032484 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 13.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 88         |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 169        |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 66         |\n",
      "|    reward             | -4.0781813 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 7.57       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 88         |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 174        |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | 41.8       |\n",
      "|    reward             | 0.28649163 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.26       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 88        |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 179       |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 354       |\n",
      "|    reward             | 1.1234958 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 118       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 89       |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 185      |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.5    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 161      |\n",
      "|    reward             | 4.112346 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 24.6     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 88         |\n",
      "|    iterations         | 3400       |\n",
      "|    time_elapsed       | 191        |\n",
      "|    total_timesteps    | 17000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3399       |\n",
      "|    policy_loss        | -124       |\n",
      "|    reward             | 0.57061017 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 9.19       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 88         |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 197        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | -13.9      |\n",
      "|    reward             | 0.14395896 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.813      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 88        |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 202       |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -142      |\n",
      "|    reward             | -5.557117 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 12.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 207       |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | 48.5      |\n",
      "|    reward             | 2.4935126 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.68      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 213       |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 105       |\n",
      "|    reward             | 2.4927285 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 218       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | 258       |\n",
      "|    reward             | 2.5054104 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 57.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 224       |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -754      |\n",
      "|    reward             | 7.5080028 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 291       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 89       |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 229      |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | 157      |\n",
      "|    reward             | 2.080538 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 18.3     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 235       |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0.00632   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | 25.1      |\n",
      "|    reward             | 0.2713448 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.929     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 241        |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | 0.0422     |\n",
      "|    reward             | -0.1489949 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.518      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 246        |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0.0259     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | -64.3      |\n",
      "|    reward             | -0.4586784 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.61       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 89          |\n",
      "|    iterations         | 4500        |\n",
      "|    time_elapsed       | 251         |\n",
      "|    total_timesteps    | 22500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4499        |\n",
      "|    policy_loss        | -37.9       |\n",
      "|    reward             | -0.20622925 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 3.81        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 257        |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | -405       |\n",
      "|    reward             | -1.2894441 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 106        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 262        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | 73.4       |\n",
      "|    reward             | -2.2362134 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.29       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 268        |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | 21.8       |\n",
      "|    reward             | 0.85408837 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.578      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 89          |\n",
      "|    iterations         | 4900        |\n",
      "|    time_elapsed       | 274         |\n",
      "|    total_timesteps    | 24500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4899        |\n",
      "|    policy_loss        | 106         |\n",
      "|    reward             | -0.20637031 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 11          |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 279       |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | -189      |\n",
      "|    reward             | 1.9920444 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 32.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 284        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | -194       |\n",
      "|    reward             | 0.08451002 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 23.6       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 89          |\n",
      "|    iterations         | 5200        |\n",
      "|    time_elapsed       | 290         |\n",
      "|    total_timesteps    | 26000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5199        |\n",
      "|    policy_loss        | 366         |\n",
      "|    reward             | -0.45257944 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 102         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 296       |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | -5.48e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | 945       |\n",
      "|    reward             | 1.6132126 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 600       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 5400      |\n",
      "|    time_elapsed       | 301       |\n",
      "|    total_timesteps    | 27000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0.0167    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5399      |\n",
      "|    policy_loss        | 183       |\n",
      "|    reward             | 1.4610767 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 22.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 307       |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | -196      |\n",
      "|    reward             | -4.648648 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 29.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 312       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -11       |\n",
      "|    reward             | 1.9107972 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.717     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 317       |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | 135       |\n",
      "|    reward             | 2.2840114 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 13.9      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 89       |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 322      |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | 216      |\n",
      "|    reward             | 2.505951 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 52.3     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 327       |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | 443       |\n",
      "|    reward             | 2.4884343 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 175       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 332        |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | 327        |\n",
      "|    reward             | -4.0722427 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 154        |\n",
      "--------------------------------------\n",
      "day: 3352, episode: 10\n",
      "begin_total_asset: 1029687.19\n",
      "end_total_asset: 8915043.19\n",
      "total_reward: 7885356.00\n",
      "total_cost: 21042.93\n",
      "total_trades: 49176\n",
      "Sharpe: 0.891\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 338        |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | 82.2       |\n",
      "|    reward             | 0.56242216 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.58       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 344       |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | -87.9     |\n",
      "|    reward             | 3.4076138 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 9.81      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 90          |\n",
      "|    iterations         | 6300        |\n",
      "|    time_elapsed       | 349         |\n",
      "|    total_timesteps    | 31500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6299        |\n",
      "|    policy_loss        | 49          |\n",
      "|    reward             | -0.43251556 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 2.16        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 89       |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 355      |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | 207      |\n",
      "|    reward             | 1.171606 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 30.4     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 361       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | 474       |\n",
      "|    reward             | 12.231841 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 392       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 366       |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | 953       |\n",
      "|    reward             | 14.564096 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 540       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 6700       |\n",
      "|    time_elapsed       | 371        |\n",
      "|    total_timesteps    | 33500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0.00261    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6699       |\n",
      "|    policy_loss        | -1.46e+03  |\n",
      "|    reward             | -31.711262 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.76e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 376       |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | 33.1      |\n",
      "|    reward             | 2.0723357 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 3.39      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 381       |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | -104      |\n",
      "|    reward             | 1.8087049 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 8.1       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 386       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | -281      |\n",
      "|    reward             | 12.268497 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 42.9      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 90       |\n",
      "|    iterations         | 7100     |\n",
      "|    time_elapsed       | 391      |\n",
      "|    total_timesteps    | 35500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7099     |\n",
      "|    policy_loss        | 112      |\n",
      "|    reward             | 5.289378 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 7.78     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 91         |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 395        |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | -2.08e+03  |\n",
      "|    reward             | -0.7797554 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 2.65e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 91         |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 400        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | -123       |\n",
      "|    reward             | -1.1105975 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 180        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 91        |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 405       |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | -119      |\n",
      "|    reward             | 1.0214945 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 9.79      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 91         |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 410        |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | -421       |\n",
      "|    reward             | 0.22747356 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 139        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 91       |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 414      |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 0.11     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | 103      |\n",
      "|    reward             | -2.12853 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 9.72     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 91         |\n",
      "|    iterations         | 7700       |\n",
      "|    time_elapsed       | 420        |\n",
      "|    total_timesteps    | 38500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | -0.0938    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7699       |\n",
      "|    policy_loss        | 292        |\n",
      "|    reward             | -1.0291935 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 92.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 91         |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 425        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | -430       |\n",
      "|    reward             | -1.6274366 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 129        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 91        |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 431       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | 44        |\n",
      "|    reward             | 17.31811  |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 6.81      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 91        |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 436       |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | -182      |\n",
      "|    reward             | -5.459565 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 22.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 91        |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 441       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | 77.8      |\n",
      "|    reward             | 0.4039861 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 3.96      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 91        |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 447       |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | 32.2      |\n",
      "|    reward             | 0.9689767 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.13      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 91        |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 452       |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | -33.9     |\n",
      "|    reward             | 4.2849994 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 5.48      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 91          |\n",
      "|    iterations         | 8400        |\n",
      "|    time_elapsed       | 457         |\n",
      "|    total_timesteps    | 42000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8399        |\n",
      "|    policy_loss        | 40.9        |\n",
      "|    reward             | -0.31287676 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 17.2        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 91        |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 462       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | 446       |\n",
      "|    reward             | -5.133169 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 207       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 91        |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 467       |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | -2.97e+03 |\n",
      "|    reward             | 54.635742 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 5.13e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 91         |\n",
      "|    iterations         | 8700       |\n",
      "|    time_elapsed       | 473        |\n",
      "|    total_timesteps    | 43500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8699       |\n",
      "|    policy_loss        | -219       |\n",
      "|    reward             | -20.966887 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 549        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 91         |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 478        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | 19.7       |\n",
      "|    reward             | 0.39309812 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.45       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 91        |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 484       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | -40.6     |\n",
      "|    reward             | 0.7793168 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.8       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 91         |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 489        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 3.76e-06   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | -289       |\n",
      "|    reward             | -1.0327978 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 56.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 92         |\n",
      "|    iterations         | 9100       |\n",
      "|    time_elapsed       | 494        |\n",
      "|    total_timesteps    | 45500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0.00402    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9099       |\n",
      "|    policy_loss        | -55.8      |\n",
      "|    reward             | 0.24393146 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 8.72       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 92        |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 498       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | -409      |\n",
      "|    reward             | 6.3508277 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 130       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 92         |\n",
      "|    iterations         | 9300       |\n",
      "|    time_elapsed       | 502        |\n",
      "|    total_timesteps    | 46500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9299       |\n",
      "|    policy_loss        | -752       |\n",
      "|    reward             | -6.7359276 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 422        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 92        |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 507       |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | -0.0226   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | -96.4     |\n",
      "|    reward             | 3.3193796 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 6.42      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 92          |\n",
      "|    iterations         | 9500        |\n",
      "|    time_elapsed       | 511         |\n",
      "|    total_timesteps    | 47500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42         |\n",
      "|    explained_variance | -0.00483    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9499        |\n",
      "|    policy_loss        | -142        |\n",
      "|    reward             | -0.86411047 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 12.3        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 516        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | -4.95      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | 103        |\n",
      "|    reward             | 0.82248825 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 6.57       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 520       |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0.00666   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | 271       |\n",
      "|    reward             | -2.209251 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 54.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 526       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 17.6      |\n",
      "|    reward             | -5.367373 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 5.52      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 93         |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 531        |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | -105       |\n",
      "|    reward             | -37.541626 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 71.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 93        |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 536       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | -58.6     |\n",
      "|    reward             | -8.597101 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 287       |\n",
      "-------------------------------------\n",
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 52        |\n",
      "|    time_elapsed    | 254       |\n",
      "|    total_timesteps | 13412     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 42        |\n",
      "|    critic_loss     | 5.76e+03  |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 10059     |\n",
      "|    reward          | -6.280161 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 45        |\n",
      "|    time_elapsed    | 587       |\n",
      "|    total_timesteps | 26824     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.04     |\n",
      "|    critic_loss     | 45.7      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 23471     |\n",
      "|    reward          | -6.280161 |\n",
      "----------------------------------\n",
      "day: 3352, episode: 10\n",
      "begin_total_asset: 996596.42\n",
      "end_total_asset: 5484174.07\n",
      "total_reward: 4487577.65\n",
      "total_cost: 1056.75\n",
      "total_trades: 46972\n",
      "Sharpe: 0.790\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 41        |\n",
      "|    time_elapsed    | 959       |\n",
      "|    total_timesteps | 40236     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.71     |\n",
      "|    critic_loss     | 6.71      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 36883     |\n",
      "|    reward          | -6.280161 |\n",
      "----------------------------------\n",
      "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 0.0001, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 85         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 23         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.02366045 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 83          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015126967 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.1       |\n",
      "|    explained_variance   | -0.00174    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 6.1         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0278     |\n",
      "|    reward               | 1.5810474   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 13.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013435087 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.00612     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 33.5        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    reward               | 6.956568    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 86.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012534836 |\n",
      "|    clip_fraction        | 0.0935      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0105     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 29.9        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    reward               | 2.091714    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 63.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010652414 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.0155      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    reward               | 1.3027297   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 35.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 84          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 144         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009841003 |\n",
      "|    clip_fraction        | 0.0804      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.00115     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 36.4        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    reward               | 2.2521398   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 76.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 167         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012017894 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0227      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 7.75        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    reward               | 1.4629854   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 13.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 85          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014846148 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.022      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 24.1        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    reward               | 0.23600124  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 67.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 213         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010247644 |\n",
      "|    clip_fraction        | 0.0904      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0019      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 32.3        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    reward               | 0.11793043  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 80.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 237         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015903154 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0041     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 12.7        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    reward               | -0.13428108 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 34.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 261         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013716569 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.00452    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 68.7        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    reward               | 1.6478548   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 141         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 283         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017974675 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0112     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 20.6        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    reward               | -1.1287969  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 38.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 306         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013686212 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.00178    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 125         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    reward               | 5.327731    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 215         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 328         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015114397 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.0102      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 111         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    reward               | 0.72321427  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 155         |\n",
      "-----------------------------------------\n",
      "day: 3352, episode: 10\n",
      "begin_total_asset: 1020829.64\n",
      "end_total_asset: 3837975.66\n",
      "total_reward: 2817146.02\n",
      "total_cost: 446354.24\n",
      "total_trades: 91354\n",
      "Sharpe: 0.656\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 352         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012747394 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -0.0488     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 25.5        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    reward               | 0.008240223 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 71.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 86          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 376         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014158466 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.00135     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 21.2        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    reward               | -3.8413067  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 79          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 398         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014563304 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -0.00403    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 19.8        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    reward               | 2.6110184   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 41.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 420         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014551129 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | 0.00603     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 24.4        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    reward               | -3.0434892  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 161         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 443         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016413648 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | 0.00365     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 75.7        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    reward               | -0.32682106 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 105         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 87         |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 468        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01657321 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.5      |\n",
      "|    explained_variance   | 0.016      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 9.73       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0208    |\n",
      "|    reward               | 0.39085588 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 23.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 492         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014660893 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.000927   |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 34.1        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    reward               | -35.23542   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 99.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 87          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 516         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017471641 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | 0.00162     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 23.5        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    reward               | -4.084444   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 72.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 542          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.01456616   |\n",
      "|    clip_fraction        | 0.173        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.6        |\n",
      "|    explained_variance   | 0.00921      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 57.6         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0223      |\n",
      "|    reward               | -0.020412523 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 76.6         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 86         |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 566        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.0161546  |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.6      |\n",
      "|    explained_variance   | -0.00452   |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 74.6       |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    reward               | -2.1065276 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 143        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 86           |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 588          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0149910785 |\n",
      "|    clip_fraction        | 0.168        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.6        |\n",
      "|    explained_variance   | -0.0305      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 9.34         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0188      |\n",
      "|    reward               | -0.5834192   |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 18.8         |\n",
      "------------------------------------------\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.00015, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to results/sac\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 31         |\n",
      "|    time_elapsed    | 428        |\n",
      "|    total_timesteps | 13412      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 47.8       |\n",
      "|    critic_loss     | 400        |\n",
      "|    ent_coef        | 0.0315     |\n",
      "|    ent_coef_loss   | -165       |\n",
      "|    learning_rate   | 0.00015    |\n",
      "|    n_updates       | 13311      |\n",
      "|    reward          | -6.1303573 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 8          |\n",
      "|    fps             | 30         |\n",
      "|    time_elapsed    | 884        |\n",
      "|    total_timesteps | 26824      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 39.3       |\n",
      "|    critic_loss     | 105        |\n",
      "|    ent_coef        | 0.0045     |\n",
      "|    ent_coef_loss   | -151       |\n",
      "|    learning_rate   | 0.00015    |\n",
      "|    n_updates       | 26723      |\n",
      "|    reward          | -13.095647 |\n",
      "-----------------------------------\n",
      "day: 3352, episode: 10\n",
      "begin_total_asset: 1013245.82\n",
      "end_total_asset: 10170185.34\n",
      "total_reward: 9156939.52\n",
      "total_cost: 3465.43\n",
      "total_trades: 55157\n",
      "Sharpe: 0.974\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 30        |\n",
      "|    time_elapsed    | 1314      |\n",
      "|    total_timesteps | 40236     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 16.9      |\n",
      "|    critic_loss     | 26.3      |\n",
      "|    ent_coef        | 0.00215   |\n",
      "|    ent_coef_loss   | -8.57     |\n",
      "|    learning_rate   | 0.00015   |\n",
      "|    n_updates       | 40135     |\n",
      "|    reward          | -8.075889 |\n",
      "----------------------------------\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0008}\n",
      "Using cpu device\n",
      "Logging to results/td3\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 46         |\n",
      "|    time_elapsed    | 290        |\n",
      "|    total_timesteps | 13412      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 131        |\n",
      "|    critic_loss     | 7.44e+03   |\n",
      "|    learning_rate   | 0.0008     |\n",
      "|    n_updates       | 10059      |\n",
      "|    reward          | -7.7322454 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 8          |\n",
      "|    fps             | 42         |\n",
      "|    time_elapsed    | 627        |\n",
      "|    total_timesteps | 26824      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 76.2       |\n",
      "|    critic_loss     | 2.96e+03   |\n",
      "|    learning_rate   | 0.0008     |\n",
      "|    n_updates       | 23471      |\n",
      "|    reward          | -7.7322454 |\n",
      "-----------------------------------\n",
      "day: 3352, episode: 10\n",
      "begin_total_asset: 989223.64\n",
      "end_total_asset: 7439635.49\n",
      "total_reward: 6450411.85\n",
      "total_cost: 988.23\n",
      "total_trades: 36872\n",
      "Sharpe: 0.909\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 12         |\n",
      "|    fps             | 41         |\n",
      "|    time_elapsed    | 962        |\n",
      "|    total_timesteps | 40236      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 53.3       |\n",
      "|    critic_loss     | 142        |\n",
      "|    learning_rate   | 0.0008     |\n",
      "|    n_updates       | 36883      |\n",
      "|    reward          | -7.7322454 |\n",
      "-----------------------------------\n",
      "hit end!\n",
      "hit end!\n",
      "hit end!\n",
      "hit end!\n",
      "hit end!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nik/.local/share/virtualenvs/FinRL-4cFawCPP/lib/python3.10/site-packages/yfinance/utils.py:775: FutureWarning: The 'unit' keyword in TimedeltaIndex construction is deprecated and will be removed in a future version. Use pd.to_timedelta instead.\n",
      "  df.index += _pd.TimedeltaIndex(dst_error_hours, 'h')\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (22, 8)\n",
      "i:  3\n",
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 88          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0.0948      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 53.3        |\n",
      "|    reward             | -0.12997587 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 2.96        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 82         |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 113        |\n",
      "|    reward             | 0.03207861 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 8.03       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 82        |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 104       |\n",
      "|    reward             | -5.153572 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 10.6      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 82          |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | 49.2        |\n",
      "|    reward             | -0.53693163 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 2.12        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 81       |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -70.7    |\n",
      "|    reward             | 1.192088 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 4.43     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 81        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -549      |\n",
      "|    reward             | 12.733852 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 198       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 82        |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0.288     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 12.3      |\n",
      "|    reward             | -1.290709 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.857     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 84          |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 47          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -107        |\n",
      "|    reward             | -0.85755193 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 7.62        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | -0.258     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -28.4      |\n",
      "|    reward             | 0.62766486 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.69       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 58         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.174      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 43         |\n",
      "|    reward             | 0.43831393 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.21       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 100       |\n",
      "|    reward             | 25.749819 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.93      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 85       |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 169      |\n",
      "|    reward             | 4.886204 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 18.1     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 84         |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 76         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0.037      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 111        |\n",
      "|    reward             | -2.5135903 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 12.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 84         |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 82         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0.0909     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 101        |\n",
      "|    reward             | -3.1334305 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.15       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 84        |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 89        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 204       |\n",
      "|    reward             | 2.0775898 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 23.4      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 84          |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 95          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.5       |\n",
      "|    explained_variance | 0.0405      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -63.5       |\n",
      "|    reward             | -0.37718704 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.7         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 6.86       |\n",
      "|    reward             | -1.0223205 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.0647     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 107         |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | 89.2        |\n",
      "|    reward             | -0.93897444 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 6.36        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 113         |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 775         |\n",
      "|    reward             | -0.82154405 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 438         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 83         |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 119        |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | -2.98      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -274       |\n",
      "|    reward             | -0.5317355 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 69.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 2100        |\n",
      "|    time_elapsed       | 125         |\n",
      "|    total_timesteps    | 10500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | -2.75       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2099        |\n",
      "|    policy_loss        | 11.2        |\n",
      "|    reward             | -0.94354236 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.441       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 83        |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | -81       |\n",
      "|    reward             | 2.1707118 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4.06      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 82       |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 138      |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | -3.99    |\n",
      "|    reward             | 1.402764 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 7.51     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 82         |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 144        |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | -57.6      |\n",
      "|    reward             | 0.38197708 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.64       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 82        |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 151       |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | 163       |\n",
      "|    reward             | 1.8778039 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 17.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 82        |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 156       |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | 462       |\n",
      "|    reward             | 1.5164495 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 174       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 82       |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 163      |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | -0.79    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | 24.5     |\n",
      "|    reward             | -1.91407 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.803    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 82        |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 169       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0.0117    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 19.9      |\n",
      "|    reward             | -1.474451 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4.19      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 82          |\n",
      "|    iterations         | 2900        |\n",
      "|    time_elapsed       | 175         |\n",
      "|    total_timesteps    | 14500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2899        |\n",
      "|    policy_loss        | -39.8       |\n",
      "|    reward             | -0.37669805 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 1.46        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 82       |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 181      |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | -42.6    |\n",
      "|    reward             | -3.44842 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.33     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 82        |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 187       |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | 49.7      |\n",
      "|    reward             | 3.0978122 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 11.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 82         |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 193        |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -42.7      |\n",
      "|    reward             | -0.5287939 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.53       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 82         |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 199        |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | -239       |\n",
      "|    reward             | -1.3373588 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 32         |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 82          |\n",
      "|    iterations         | 3400        |\n",
      "|    time_elapsed       | 205         |\n",
      "|    total_timesteps    | 17000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 5.7e-05     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3399        |\n",
      "|    policy_loss        | -119        |\n",
      "|    reward             | -0.44706362 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 11.4        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 82         |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 211        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 41.8       |\n",
      "|    reward             | 0.55779046 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.06       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 82          |\n",
      "|    iterations         | 3600        |\n",
      "|    time_elapsed       | 218         |\n",
      "|    total_timesteps    | 18000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | -0.0311     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3599        |\n",
      "|    policy_loss        | -233        |\n",
      "|    reward             | -0.61513394 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 35.6        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 82        |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 224       |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | -14.1     |\n",
      "|    reward             | 1.6885079 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.12      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 82       |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 230      |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 197      |\n",
      "|    reward             | -2.20824 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 25.8     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 82        |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 236       |\n",
      "|    total_timesteps    | 19500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | 190       |\n",
      "|    reward             | -8.647753 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 25.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 82       |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 242      |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | -14.8    |\n",
      "|    reward             | 1.356411 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.27     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 82         |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 248        |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | -41        |\n",
      "|    reward             | 0.82959485 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.5        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 82        |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 254       |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | 32.9      |\n",
      "|    reward             | 0.7610502 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.73      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 82        |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 259       |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | 21.9      |\n",
      "|    reward             | 2.1365376 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.92      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 82          |\n",
      "|    iterations         | 4400        |\n",
      "|    time_elapsed       | 265         |\n",
      "|    total_timesteps    | 22000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | -0.000325   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4399        |\n",
      "|    policy_loss        | 35.3        |\n",
      "|    reward             | -0.29432794 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 2.62        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 83        |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 271       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | -6.18     |\n",
      "|    reward             | 2.3219092 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 13.7      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 4600        |\n",
      "|    time_elapsed       | 276         |\n",
      "|    total_timesteps    | 23000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | -0.00817    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4599        |\n",
      "|    policy_loss        | -411        |\n",
      "|    reward             | -0.53025675 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 116         |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 83           |\n",
      "|    iterations         | 4700         |\n",
      "|    time_elapsed       | 282          |\n",
      "|    total_timesteps    | 23500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.7        |\n",
      "|    explained_variance | 0.0145       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4699         |\n",
      "|    policy_loss        | -26.2        |\n",
      "|    reward             | -0.037039604 |\n",
      "|    std                | 1.02         |\n",
      "|    value_loss         | 1.88         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 83        |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 287       |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | -253      |\n",
      "|    reward             | 1.0264853 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 35.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 83        |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 293       |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0.154     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -87.1     |\n",
      "|    reward             | 4.7489624 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.47      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 83        |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 298       |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | -18.5     |\n",
      "|    reward             | 3.5302439 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.55      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 83        |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 304       |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | 333       |\n",
      "|    reward             | 1.7802906 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 76        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 83        |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 310       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | 113       |\n",
      "|    reward             | 3.9753313 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 16.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 83       |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 316      |\n",
      "|    total_timesteps    | 26500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | 422      |\n",
      "|    reward             | 6.916476 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 111      |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 83          |\n",
      "|    iterations         | 5400        |\n",
      "|    time_elapsed       | 321         |\n",
      "|    total_timesteps    | 27000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0.122       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5399        |\n",
      "|    policy_loss        | -189        |\n",
      "|    reward             | -0.57704854 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 19.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 84         |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 327        |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | 24         |\n",
      "|    reward             | -0.9598218 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.507      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 84        |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 332       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | 65.2      |\n",
      "|    reward             | 1.7944642 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.99      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 84         |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 337        |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | -4.89      |\n",
      "|    reward             | -2.9123693 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.674      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 84        |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 343       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 131       |\n",
      "|    reward             | -0.701842 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 11.3      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 84       |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 349      |\n",
      "|    total_timesteps    | 29500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | -135     |\n",
      "|    reward             | 2.748403 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 12.7     |\n",
      "------------------------------------\n",
      "day: 3330, episode: 10\n",
      "begin_total_asset: 987995.69\n",
      "end_total_asset: 3899665.58\n",
      "total_reward: 2911669.89\n",
      "total_cost: 35527.11\n",
      "total_trades: 57277\n",
      "Sharpe: 0.677\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 84        |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 354       |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | 18.5      |\n",
      "|    reward             | 1.4335545 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.376     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 84          |\n",
      "|    iterations         | 6100        |\n",
      "|    time_elapsed       | 360         |\n",
      "|    total_timesteps    | 30500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0.117       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6099        |\n",
      "|    policy_loss        | 128         |\n",
      "|    reward             | -0.34496605 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 12.6        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 84       |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 365      |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | -285     |\n",
      "|    reward             | 3.348351 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 107      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 84         |\n",
      "|    iterations         | 6300       |\n",
      "|    time_elapsed       | 370        |\n",
      "|    total_timesteps    | 31500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6299       |\n",
      "|    policy_loss        | -165       |\n",
      "|    reward             | 0.14470439 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 16.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 376       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | -179      |\n",
      "|    reward             | -8.418529 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 20.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 85          |\n",
      "|    iterations         | 6500        |\n",
      "|    time_elapsed       | 381         |\n",
      "|    total_timesteps    | 32500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6499        |\n",
      "|    policy_loss        | 195         |\n",
      "|    reward             | -0.13473868 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 24          |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 85       |\n",
      "|    iterations         | 6600     |\n",
      "|    time_elapsed       | 386      |\n",
      "|    total_timesteps    | 33000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | 202      |\n",
      "|    reward             | 10.77827 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 37.6     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 392       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | -63.7     |\n",
      "|    reward             | 0.7546126 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.5       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 398       |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0.0346    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | 114       |\n",
      "|    reward             | 3.1131222 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 11.2      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 85          |\n",
      "|    iterations         | 6900        |\n",
      "|    time_elapsed       | 403         |\n",
      "|    total_timesteps    | 34500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.7       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6899        |\n",
      "|    policy_loss        | 34.5        |\n",
      "|    reward             | -0.11851611 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 1.98        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 409       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | 8.46      |\n",
      "|    reward             | 1.1590991 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 9.7       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 415       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 271       |\n",
      "|    reward             | 4.845641  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 50.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 421       |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | 552       |\n",
      "|    reward             | 4.076469  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 210       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 427       |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | 409       |\n",
      "|    reward             | 19.888315 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 284       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 433       |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0.501     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | 44.1      |\n",
      "|    reward             | 1.1619684 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.92      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 438        |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | -50.2      |\n",
      "|    reward             | 0.09465884 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.51       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 7600       |\n",
      "|    time_elapsed       | 444        |\n",
      "|    total_timesteps    | 38000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7599       |\n",
      "|    policy_loss        | 47.1       |\n",
      "|    reward             | 0.40150747 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.89       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 7700       |\n",
      "|    time_elapsed       | 449        |\n",
      "|    total_timesteps    | 38500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7699       |\n",
      "|    policy_loss        | -19.6      |\n",
      "|    reward             | -0.8631953 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.878      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 85          |\n",
      "|    iterations         | 7800        |\n",
      "|    time_elapsed       | 454         |\n",
      "|    total_timesteps    | 39000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7799        |\n",
      "|    policy_loss        | 134         |\n",
      "|    reward             | -0.44072965 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 10.4        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 460       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | 752       |\n",
      "|    reward             | -2.954479 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 552       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 85          |\n",
      "|    iterations         | 8000        |\n",
      "|    time_elapsed       | 465         |\n",
      "|    total_timesteps    | 40000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7999        |\n",
      "|    policy_loss        | -10.2       |\n",
      "|    reward             | -0.40505132 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.37        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 471       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | -121      |\n",
      "|    reward             | 1.3115553 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 8.79      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 86        |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 476       |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | 131       |\n",
      "|    reward             | 1.0615321 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 10.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 482        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0.00842    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | -148       |\n",
      "|    reward             | -4.9340568 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 16.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 8400      |\n",
      "|    time_elapsed       | 488       |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8399      |\n",
      "|    policy_loss        | 183       |\n",
      "|    reward             | 2.0251622 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 25.4      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 85       |\n",
      "|    iterations         | 8500     |\n",
      "|    time_elapsed       | 495      |\n",
      "|    total_timesteps    | 42500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8499     |\n",
      "|    policy_loss        | 68.5     |\n",
      "|    reward             | 1.991648 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 13.9     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 501        |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | 1.43e+03   |\n",
      "|    reward             | -15.271592 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.45e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 8700       |\n",
      "|    time_elapsed       | 507        |\n",
      "|    total_timesteps    | 43500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | -0.199     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8699       |\n",
      "|    policy_loss        | -36.2      |\n",
      "|    reward             | -2.2162025 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.954      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 514       |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | 51        |\n",
      "|    reward             | 1.6954606 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 3.33      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 520       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | -90.4     |\n",
      "|    reward             | 0.6846004 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 5.77      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 85          |\n",
      "|    iterations         | 9000        |\n",
      "|    time_elapsed       | 526         |\n",
      "|    total_timesteps    | 45000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.1       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8999        |\n",
      "|    policy_loss        | 79          |\n",
      "|    reward             | -0.14568944 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 16.5        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 85       |\n",
      "|    iterations         | 9100     |\n",
      "|    time_elapsed       | 532      |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9099     |\n",
      "|    policy_loss        | 21.9     |\n",
      "|    reward             | 0.990469 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 6.26     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 85          |\n",
      "|    iterations         | 9200        |\n",
      "|    time_elapsed       | 537         |\n",
      "|    total_timesteps    | 46000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.1       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9199        |\n",
      "|    policy_loss        | 283         |\n",
      "|    reward             | -0.32960603 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 73.1        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 9300       |\n",
      "|    time_elapsed       | 543        |\n",
      "|    total_timesteps    | 46500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9299       |\n",
      "|    policy_loss        | 2.02e+03   |\n",
      "|    reward             | -15.695733 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.68e+03   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 85          |\n",
      "|    iterations         | 9400        |\n",
      "|    time_elapsed       | 548         |\n",
      "|    total_timesteps    | 47000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.1       |\n",
      "|    explained_variance | 0.0499      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9399        |\n",
      "|    policy_loss        | 17.1        |\n",
      "|    reward             | -0.10270636 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.95        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 85       |\n",
      "|    iterations         | 9500     |\n",
      "|    time_elapsed       | 554      |\n",
      "|    total_timesteps    | 47500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | 68.3     |\n",
      "|    reward             | 3.539611 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 3.66     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 559        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -250       |\n",
      "|    reward             | 0.53393984 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 37.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 85         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 565        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | -0.00551   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | -72.1      |\n",
      "|    reward             | -0.5177336 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 6.13       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 85       |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 570      |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | 73.3     |\n",
      "|    reward             | 1.715799 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 9.02     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 85        |\n",
      "|    iterations         | 9900      |\n",
      "|    time_elapsed       | 576       |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.2     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | 152       |\n",
      "|    reward             | 10.402426 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 41.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 86         |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 581        |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | -58.8      |\n",
      "|    reward             | 0.83877695 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 3.63       |\n",
      "--------------------------------------\n",
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 45        |\n",
      "|    time_elapsed    | 295       |\n",
      "|    total_timesteps | 13324     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -23       |\n",
      "|    critic_loss     | 38.3      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 9993      |\n",
      "|    reward          | -8.113274 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 40        |\n",
      "|    time_elapsed    | 653       |\n",
      "|    total_timesteps | 26648     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -14.1     |\n",
      "|    critic_loss     | 6.44      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 23317     |\n",
      "|    reward          | -8.113274 |\n",
      "----------------------------------\n",
      "day: 3330, episode: 10\n",
      "begin_total_asset: 947468.49\n",
      "end_total_asset: 6829011.10\n",
      "total_reward: 5881542.60\n",
      "total_cost: 946.52\n",
      "total_trades: 43290\n",
      "Sharpe: 0.885\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 38        |\n",
      "|    time_elapsed    | 1026      |\n",
      "|    total_timesteps | 39972     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -12.6     |\n",
      "|    critic_loss     | 3.31      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 36641     |\n",
      "|    reward          | -8.113274 |\n",
      "----------------------------------\n",
      "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 0.0001, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 92         |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 22         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | -0.3609186 |\n",
      "-----------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 95         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 42         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01143479 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.2      |\n",
      "|    explained_variance   | -0.00624   |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 4.11       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0223    |\n",
      "|    reward               | 0.5247674  |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 10.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010560924 |\n",
      "|    clip_fraction        | 0.0905      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00373    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 53.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    reward               | -24.032772  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 91.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 91           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 89           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0120746475 |\n",
      "|    clip_fraction        | 0.0709       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.2        |\n",
      "|    explained_variance   | 6.38e-05     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 22.8         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0153      |\n",
      "|    reward               | 0.8311901    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 62.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 112         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011248414 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.00159     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 11.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    reward               | 0.4433757   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 45.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011731452 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0135     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 34.9        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    reward               | 5.5393806   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 83.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012525571 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0505     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 13.1        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    reward               | -1.4394498  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 26.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 181         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012923483 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00654    |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 46.7        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    reward               | 2.7013047   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 168         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 204          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0136833405 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.2        |\n",
      "|    explained_variance   | -0.00283     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 54.4         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0151      |\n",
      "|    reward               | -0.81135714  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 88.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 227         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014707629 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | 0.00837     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 33.4        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    reward               | 0.589554    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 66.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 249         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015155762 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00737     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 162         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0217     |\n",
      "|    reward               | 0.95484     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 205         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 272          |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0129773505 |\n",
      "|    clip_fraction        | 0.114        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.3        |\n",
      "|    explained_variance   | -0.00926     |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 23.5         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0195      |\n",
      "|    reward               | 2.9603899    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 42           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 295         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013168335 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0164      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 41.7        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    reward               | 7.663986    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 139         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 317          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0115534095 |\n",
      "|    clip_fraction        | 0.0937       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.3        |\n",
      "|    explained_variance   | 0.00804      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 47.9         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.0145      |\n",
      "|    reward               | 2.5386298    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n",
      "day: 3330, episode: 10\n",
      "begin_total_asset: 1012390.94\n",
      "end_total_asset: 4368775.42\n",
      "total_reward: 3356384.49\n",
      "total_cost: 407417.17\n",
      "total_trades: 89724\n",
      "Sharpe: 0.654\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 339          |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.014248043  |\n",
      "|    clip_fraction        | 0.175        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.3        |\n",
      "|    explained_variance   | 0.0291       |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 11.5         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0194      |\n",
      "|    reward               | -0.047805775 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 27           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 362         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012426896 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.011       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 45.5        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    reward               | 3.7650247   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 156         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 90         |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 385        |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01161271 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.3      |\n",
      "|    explained_variance   | -0.0105    |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 32.2       |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    reward               | 1.3292725  |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 72.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 404         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013507681 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.017       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 40.3        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    reward               | 0.23966528  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 79.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 423         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012394716 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0763     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 38.3        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    reward               | 3.650058    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 91.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 443         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017375901 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0113      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    reward               | 0.68542045  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 19.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 92          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 463         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017430175 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.00513     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 46.6        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    reward               | -0.18999167 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 93.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 484         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011452854 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0263     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 26.8        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    reward               | 0.3808311   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 55.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 504         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016496813 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | 0.0068      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 18.8        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    reward               | 0.34294865  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 57.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 526         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017709084 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0101     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 24.4        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    reward               | 1.8507787   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 89.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 93          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 547         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020453185 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0818     |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 10.9        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    reward               | 0.36692747  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 26.1        |\n",
      "-----------------------------------------\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.00015, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to results/sac\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 34         |\n",
      "|    time_elapsed    | 380        |\n",
      "|    total_timesteps | 13324      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 475        |\n",
      "|    critic_loss     | 93.8       |\n",
      "|    ent_coef        | 0.0527     |\n",
      "|    ent_coef_loss   | -133       |\n",
      "|    learning_rate   | 0.00015    |\n",
      "|    n_updates       | 13223      |\n",
      "|    reward          | -3.8125093 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 8          |\n",
      "|    fps             | 34         |\n",
      "|    time_elapsed    | 766        |\n",
      "|    total_timesteps | 26648      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 237        |\n",
      "|    critic_loss     | 62.7       |\n",
      "|    ent_coef        | 0.00821    |\n",
      "|    ent_coef_loss   | -69.3      |\n",
      "|    learning_rate   | 0.00015    |\n",
      "|    n_updates       | 26547      |\n",
      "|    reward          | -3.8678825 |\n",
      "-----------------------------------\n",
      "day: 3330, episode: 10\n",
      "begin_total_asset: 991140.13\n",
      "end_total_asset: 5270828.85\n",
      "total_reward: 4279688.72\n",
      "total_cost: 3412.97\n",
      "total_trades: 53727\n",
      "Sharpe: 0.756\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 12         |\n",
      "|    fps             | 34         |\n",
      "|    time_elapsed    | 1147       |\n",
      "|    total_timesteps | 39972      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 127        |\n",
      "|    critic_loss     | 36.2       |\n",
      "|    ent_coef        | 0.00388    |\n",
      "|    ent_coef_loss   | -1.71      |\n",
      "|    learning_rate   | 0.00015    |\n",
      "|    n_updates       | 39871      |\n",
      "|    reward          | -3.8799672 |\n",
      "-----------------------------------\n",
      "{'batch_size': 64, 'buffer_size': 100000, 'learning_rate': 0.0008}\n",
      "Using cpu device\n",
      "Logging to results/td3\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 48         |\n",
      "|    time_elapsed    | 272        |\n",
      "|    total_timesteps | 13324      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 12         |\n",
      "|    critic_loss     | 366        |\n",
      "|    learning_rate   | 0.0008     |\n",
      "|    n_updates       | 9993       |\n",
      "|    reward          | -5.4564238 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 8          |\n",
      "|    fps             | 44         |\n",
      "|    time_elapsed    | 601        |\n",
      "|    total_timesteps | 26648      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 21         |\n",
      "|    critic_loss     | 19.1       |\n",
      "|    learning_rate   | 0.0008     |\n",
      "|    n_updates       | 23317      |\n",
      "|    reward          | -5.4564238 |\n",
      "-----------------------------------\n",
      "day: 3330, episode: 10\n",
      "begin_total_asset: 952495.93\n",
      "end_total_asset: 5463488.51\n",
      "total_reward: 4510992.58\n",
      "total_cost: 951.54\n",
      "total_trades: 49854\n",
      "Sharpe: 0.854\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 12         |\n",
      "|    fps             | 42         |\n",
      "|    time_elapsed    | 933        |\n",
      "|    total_timesteps | 39972      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 22.3       |\n",
      "|    critic_loss     | 15.7       |\n",
      "|    learning_rate   | 0.0008     |\n",
      "|    n_updates       | 36641      |\n",
      "|    reward          | -5.4564238 |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_start_date, train_end_date = \"2009-01-01\", \"2022-07-01\"\n",
    "trade_start_date, trade_end_date = str(train_end_date), \"2022-11-01\"\n",
    "rolling_window_length = 22  # num of trading days in a rolling window\n",
    "if_store_actions = True\n",
    "if_store_result = True\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_sac = True\n",
    "if_using_td3 = True\n",
    "stock_trading_rolling_window(\n",
    "    train_start_date,\n",
    "    train_end_date,\n",
    "    trade_start_date,\n",
    "    trade_end_date,\n",
    "    rolling_window_length,\n",
    "    if_store_actions=if_store_actions,\n",
    "    if_using_a2c=if_using_a2c,\n",
    "    if_store_result=if_store_result,\n",
    "    if_using_ddpg=if_using_ddpg,\n",
    "    if_using_ppo=if_using_ppo,\n",
    "    if_using_sac=if_using_sac,\n",
    "    if_using_td3=if_using_td3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HMNR5nHjh1iz",
    "uijiWgkuh1jB",
    "MRiOtrywfAo1",
    "_gDkU-j-fCmZ",
    "3Zpv4S0-fDBv"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "54cefccbf0f07c9750f12aa115c023dfa5ed4acecf9e7ad3bc9391869be60d0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
